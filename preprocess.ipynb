{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e8f746",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fd4b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Reusing dataset code_search_net (/home/anoushkav/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c382eef344d5458bbde7756917fb69ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                                                                              | 1503/94042 [00:00<00:26, 3484.39it/s]\n",
      " 43%|██████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 38444/89154 [00:06<00:09, 5606.11it/s]\n",
      " 49%|█████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                          | 49094/100529 [00:08<00:09, 5500.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "from parser import remove_comments_and_docstrings\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Example(object):\n",
    "    def __init__(self, code, desc, lang):\n",
    "        self.code = code\n",
    "        self.desc = desc\n",
    "        self.lang = lang\n",
    "        \n",
    "def read_examples():\n",
    "    dataset = load_dataset('code_search_net')\n",
    "    langs = ['javascript']\n",
    "    \n",
    "    np.random.seed(2789)\n",
    "    train = []\n",
    "    sampled = np.random.choice(range(len(dataset['train'])), int(0.05*len(dataset['train'])), replace=False )\n",
    "    sampled = list(map(int, sampled))\n",
    "    for i in tqdm(sampled):\n",
    "        if dataset['train'][i]['language'] in langs:\n",
    "            code, desc, lang = dataset['train'][i]['func_code_string'], \\\n",
    "                        dataset['train'][i]['func_documentation_string'], dataset['train'][i]['language']\n",
    "            try:\n",
    "                code = remove_comments_and_docstrings(code, lang)\n",
    "                if lang==\"php\":\n",
    "                    if not(code.startswith('<?php')):\n",
    "                        code=\"<?php\"+code+\"?>\"\n",
    "                train.append( Example(code, desc, lang) )  \n",
    "            except:\n",
    "                pass\n",
    "            if len(train)>=100:\n",
    "                break\n",
    "            \n",
    "    valid = []\n",
    "    for sample in tqdm(dataset['validation']):\n",
    "        if sample['language'] in langs:\n",
    "            code, desc, lang = sample['func_code_string'], \\\n",
    "                        sample['func_documentation_string'], sample['language']\n",
    "            try:\n",
    "                code = remove_comments_and_docstrings(code, lang)\n",
    "                if lang==\"php\":\n",
    "                    if not(code.startswith('<?php')):\n",
    "                        code=\"<?php\"+code+\"?>\" \n",
    "                valid.append( Example(code, desc, lang) )\n",
    "            except:\n",
    "                continue \n",
    "            if len(valid)>=10:\n",
    "                break\n",
    "                \n",
    "    test = []\n",
    "    for sample in tqdm(dataset['test']):\n",
    "        if sample['language'] in langs:\n",
    "            code, desc, lang = sample['func_code_string'], \\\n",
    "                        sample['func_documentation_string'], sample['language']\n",
    "            try:\n",
    "                code = remove_comments_and_docstrings(code, lang)\n",
    "                if lang==\"php\":\n",
    "                    if not(code.startswith('<?php')):\n",
    "                        code=\"<?php\"+code+\"?>\" \n",
    "                test.append( Example(code, desc, lang) )\n",
    "            except:\n",
    "                continue  \n",
    "            if len(test)>=10:\n",
    "                break\n",
    "    return list(train), list(valid), list(test)\n",
    "\n",
    "train_examples, valid_examples, test_examples = read_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12b52c",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e235a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 5287.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 120/120 [00:00<00:00, 498.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from parser import DFG_python,DFG_java,DFG_ruby,DFG_go,DFG_php,DFG_javascript,DFG_csharp\n",
    "from parser import (remove_comments_and_docstrings,\n",
    "                   tree_to_token_index,\n",
    "                   index_to_code_token,\n",
    "                   tree_to_variable_index, \n",
    "                   detokenize_code)\n",
    "from tree_sitter import Language, Parser\n",
    "from transformers import RobertaTokenizer\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "\n",
    "\n",
    "dfg_function={\n",
    "    'python':DFG_python,\n",
    "    'java':DFG_java,\n",
    "    'php':DFG_php,\n",
    "    'javascript':DFG_javascript,\n",
    "}\n",
    "\n",
    "parsers={}        \n",
    "for lang in dfg_function:\n",
    "    LANGUAGE = Language('parser/my-languages.so', lang)\n",
    "    parser = Parser()\n",
    "    parser.set_language(LANGUAGE) \n",
    "    parser = [parser,dfg_function[lang]]    \n",
    "    parsers[lang]= parser\n",
    "    \n",
    "class InputFeatures(object):\n",
    "    def __init__(self,\n",
    "                 example_index,\n",
    "                 code_tokens_ids,\n",
    "                 desc_tokens_ids,\n",
    "                 ast_node_types, \n",
    "                 ast_adj,\n",
    "                 graph_feature\n",
    "    ):\n",
    "        self.example_index = example_index\n",
    "        self.code_tokens_ids = code_tokens_ids\n",
    "        self.desc_tokens_ids = desc_tokens_ids\n",
    "        self.ast_node_types = ast_node_types\n",
    "        self.ast_adj = ast_adj\n",
    "        self.graph_feature =  graph_feature\n",
    "    \n",
    "def gather_node_types(node):\n",
    "    global ast_node_types\n",
    "    ast_node_types.append(node.type)\n",
    "    def helper(node):\n",
    "        global ast_node_types\n",
    "        for child in node.children:\n",
    "            ast_node_types.append(child.type)\n",
    "            helper(child)\n",
    "    helper(node)\n",
    "        \n",
    "def get_leaf_nodes(node, output):\n",
    "    if node.children==[]:\n",
    "        output.append(node)\n",
    "    for node in node.children:\n",
    "        get_leaf_nodes(node, output)\n",
    "        \n",
    "def overlap(s1,e1,s2,e2):\n",
    "    if s1[0]!=e1[0]:\n",
    "        raise Exception()\n",
    "    if s1[0]==s2[0]:\n",
    "        return (s1[1]<=s2[1]<e1[1])|(s2[1]<=s1[1]<e2[1])\n",
    "    return False\n",
    "\n",
    "def get_lr_path(leaf):\n",
    "    path = [leaf]\n",
    "    while path[-1].parent is not None:\n",
    "        path.append(path[-1].parent)\n",
    "    return path\n",
    "        \n",
    "def get_ll_sim(p1, p2):\n",
    "    common = 1\n",
    "    for i in range(2, min(len(p1)-1, len(p2)-1)):\n",
    "        if p1[-i]==p2[-i]:\n",
    "            common += 1\n",
    "        else:\n",
    "            break\n",
    "    return common*common / (len(p1)*len(p2))\n",
    "\n",
    "def get_adj_list(ast):\n",
    "    node=ast.root_node\n",
    "    global adj\n",
    "    adj=defaultdict(list)\n",
    "    global counter\n",
    "    counter=0\n",
    "    def helper(node):\n",
    "        global adj\n",
    "        global counter \n",
    "        c=counter\n",
    "        for child in node.children:\n",
    "            counter+=1\n",
    "            adj[c].append(counter)\n",
    "            helper(child)\n",
    "    helper(node)\n",
    "    return adj\n",
    "\n",
    "def get_graph_feature_list(ast,dict_node_types):\n",
    "    node=ast.root_node\n",
    "    global graph_feature\n",
    "    graph_feature=[]\n",
    "    graph_feature.append(dict_node_types[node.type])\n",
    "    def helper(node):\n",
    "        global graph_feature\n",
    "        for child in node.children:\n",
    "            graph_feature.append(dict_node_types[child.type])\n",
    "            helper(child)\n",
    "    helper(node)\n",
    "    return graph_feature\n",
    "        \n",
    "          \n",
    "def convert_examples_to_features(examples, tokenizer):\n",
    "    features = []\n",
    "    global ast_node_types\n",
    "    ast_node_types = []\n",
    "    \n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "        ast = parsers[example.lang][0].parse(bytes(example.code, 'utf-8'))\n",
    "        gather_node_types(ast.root_node)\n",
    "        \n",
    "    dict_node_types=defaultdict(list)\n",
    "                        \n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(np.unique(ast_node_types))\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    \n",
    "    for idx, ntype in enumerate(np.unique(ast_node_types)):\n",
    "        dict_node_types[ntype] = onehot_encoded[idx]\n",
    "        \n",
    "    for example_index, example in enumerate(tqdm(examples,total=len(examples))):\n",
    "   \n",
    "        code_ids = tokenizer.tokenize(example.code)\n",
    "        desc_ids = tokenizer.tokenize(example.desc)\n",
    "                \n",
    "        code_tokens_ids = tokenizer.convert_tokens_to_ids(code_ids)\n",
    "        desc_tokens_ids = tokenizer.convert_tokens_to_ids(desc_ids)\n",
    "                        \n",
    "        ast = parsers[example.lang][0].parse(bytes(example.code, 'utf-8'))\n",
    "                \n",
    "        ast_adj=get_adj_list(ast)\n",
    "        \n",
    "            \n",
    "        graph_feature=get_graph_feature_list(ast,dict_node_types)     \n",
    "        \n",
    "        \n",
    "                \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 code_tokens_ids, \n",
    "                 desc_tokens_ids, \n",
    "                 ast_node_types, \n",
    "                 ast_adj,\n",
    "                 graph_feature\n",
    "            )\n",
    "        )\n",
    "    return features\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "features = convert_examples_to_features(train_examples+valid_examples+test_examples, tokenizer)\n",
    "\n",
    "pickle.dump({'train':features[:len(train_examples)], 'valid':features[len(train_examples):len(train_examples)+len(valid_examples)],'test':features[len(train_examples)+len(valid_examples):]},\n",
    "            open('features_pt_javascript.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypercode",
   "language": "python",
   "name": "hypercode"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
